{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aec3b89a",
   "metadata": {},
   "source": [
    "# <center> Découverte de Scikit-learn </center>\n",
    "\n",
    "**Source :**  MASSARON L., MUELLER J.-P., Data science avec Python pour les nuls, First, 2019, p.232 et suivantes.\n",
    "    \n",
    "## Les classes de Scikit-learn\n",
    "\n",
    "La classe ancestrale `BaseEstimator` est la mère de toutes les autres. Quatre classes couvrent toutes les possibilités d'apprentissage machine de base :\n",
    "- Classification\n",
    "- Régression\n",
    "- Regroupement en groupes (clusters)\n",
    "- Transformations\n",
    "Chaque classe de base définit ses propres méthodes et attributs. \n",
    "\n",
    "Interfaces (API) orientées objets de Sklearn : garantissent l'homogénéité des méthodes et attributs entre les différents algorithmes du paquetage. Ils sont au nombre de 4 :\n",
    "- `estimator`: pour ajuster les paramètres en les apprenants à partir des données grâce à l'algorithme\n",
    "- `predicator`: pour générer des prédictions à partir des paramètres ajustés\n",
    "- `transformator`: pour transformer des données en utilisant les paramètres ajustés \n",
    "- `model`: pour rendre compte de la qualité d'ajustement ou d'autres points de mesure.\n",
    "\n",
    "## Sélection d'applications pour la datalogie\n",
    "\n",
    "L'interface `estimator` règle les problèmes suivants :\n",
    "- **problèmes de classification :** pour estimer qu'une nouvelle observation appartient ou non à un certain groupe.\n",
    "- **problèmes de régression :** pour estimer la valeur d'une nouvelle observation.\n",
    "\n",
    "Dans ce cas précis, il s'agit d'appliquer la méthode `fit(X, y)`, `X` correspondant au tableau bidimensionnel de prédicteurs (donc le jeu d'observation à apprendre) alors que `y` est le résultat, soit un tableau à 1 dimension.\n",
    "\n",
    "En appliquant la méthode `fit()`, vous mettez en relation l'information dans `X` avec `y`. Une nouvelle donnée ayant les mêmes caractéristiques que `X` permet de déduire `y`correctement. Certains des paramètres sont estimés par la méthode `fit()` en interne. Vous pouvez ainsi distinguer les paramètres qui sont appris des hyperparamètres qui sont définis par vous au moment de créer l'instance de l'apprenant. \n",
    "\n",
    "Cette instanciation consiste à associer une classe de Sklearn à une variable Python. En plus des hyperparamètres, vous pouvez stimuler des paramètres de travail, par exemple la normalisation demandée ou la semence des valeurs aléatoires afin d'obtenir les mêmes résultats dans tous les appels travaillant sur les mêmes données d'entrée. \n",
    "\n",
    "## Exemple de régression linéaire\n",
    "\n",
    "Le dataset Boston contient des variables prédicateurs que nous pouvons confronter aux prix des maisons, afin de générer un prédicateur qui va pouvoir estimer une nouvelle maison à partir de ses caractéristiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca51d886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:(506, 13) y:(506,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "X, y = boston.data, boston.target\n",
    "print(\"X:%s y:%s\" %(X.shape, y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0d8e31",
   "metadata": {},
   "source": [
    "Les 2 tableaux ont le même nombre de lignes et `X` comporte 13 caractéristiques. La méthode `shape()` analyse un tableau et renvoie sa dimension.\n",
    "Le nombre de lignes doit être le même pour `X`et `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c206474d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.08011358e-01  4.64204584e-02  2.05586264e-02  2.68673382e+00\n",
      " -1.77666112e+01  3.80986521e+00  6.92224640e-04 -1.47556685e+00\n",
      "  3.06049479e-01 -1.23345939e-02 -9.52747232e-01  9.31168327e-03\n",
      " -5.24758378e-01]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# importation de la classe LinearRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# instanciation d'une variable + choix du mode de normalisation\n",
    "hypothesis = LinearRegression(normalize=True)\n",
    "\n",
    "# ajustement = la variable contient les paramètres appris\n",
    "hypothesis.fit(X, y)\n",
    "\n",
    "# affichage des coefficients\n",
    "print(hypothesis.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897929b5",
   "metadata": {},
   "source": [
    "Une hypothèse est une façon de décrire un algorithme qui a été entraîné avec des données. L'hypothèse définit une représentation possible de **y** en fonction de **X** que vous testez au niveau de la validité. C'est donc une hypothèse tant en termes scientifiques, qu'en termes d'apprentissage machine.\n",
    "\n",
    "### Prédicteur :\n",
    "\n",
    "La classe `predictor` sert à prédire la probabilité d'un certain résultat, en obtenant ce résultat pour les nouvelles observations avec ses méthodes `predict()` et `predict_proba()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e8080cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25.90156732]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "new_observation = np.array([1, 0, 1, 0, 0.5, 7, 59,\n",
    "                           6, 3, 200, 20, 350, 4],\n",
    "                          dtype=float).reshape(1, -1)\n",
    "\n",
    "# predict\n",
    "print(hypothesis.predict(new_observation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81f68e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7406426641094094"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# qualité de l'ajustement \n",
    "hypothesis.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894b9f3e",
   "metadata": {},
   "source": [
    "`score()` renvoie le coefficient de détermination R au carré pour la prédiction. Il s'agit d'une mesure entre 0/1 qui compare le prédicteur à une moyenne simple. Les valeurs hautes indiquent que le prédicteur fonctionne correctement. \n",
    "\n",
    "### Classe de transformation :\n",
    "Applique des transformations à d'autres tableaux de données en s'appuyant sur la phase d'ajustement. Il n'y a pas de méthode `transform` pour la régression linéaire, mais la plupart des autres algorithmes de prétraitement en disposent. \n",
    "`MinMaxScaler()` est ainsi capable de transformer les valeurs dans une plage spécifiée par une valeur minimale et une maximale, en apprenant la formule de transformation à partir d'un tableau d'exemple : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3b2b5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01116872 0.         0.01979472 0.         0.23662551 0.65893849\n",
      "  0.57775489 0.44288845 0.08695652 0.02480916 0.78723404 0.88173887\n",
      "  0.06263797]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(X)\n",
    "# on applique les valeurs min/max apprises sur X\n",
    "print(scaler.transform(new_observation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44943f8",
   "metadata": {},
   "source": [
    "---\n",
    "<br>\n",
    "\n",
    "## La technique de hachage\n",
    "Les fonctions de hachage servent à transformer n'importent quelles données d'entrée en données de sortie ayant des caractéristiques prévisibles. En général, la valeur renvoyée est liée à un intervalle particulier, dont les bornes vont par exemple d'une valeur négative à une valeur positive, ou seulement d'une valeur positive à une autre. Cela revient un peu à **appliquer un standard à vos données** : quelles que soient les valeurs fournies, le produit sera toujours le même.\n",
    "\n",
    "= fourni une valeur numérique pour une valeur d'entrée, ex : le mot 'chien' renverra toujours la même valeur numérique.\n",
    "= transforme en nombre.\n",
    "= on ne peut pas revenir à la valeur de départ à partir de celle d'arrivée. \n",
    "\n",
    "### Exemple avec Python et la fonction `hash()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d40f4a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7212491160745632632"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash('Exemple')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e3cc86",
   "metadata": {},
   "source": [
    "## Codage un sur N ou one-hot :\n",
    "\n",
    "Chaîne = 'Python for data science'\n",
    "\n",
    "1- **Affectation d'un nombre arbitraire à chaque mot**, par exemple Python=0, for=1, data et science 2 et 3.\n",
    "2- **Initialisation du vecteur en comptant le nombre de mots uniques qui ont été associés à un code dans la 1ère étape.**\n",
    "3- **Utilisation des codes de l'étape 1 comme index pour insérer des valeurs dans le vecteur, en assignant la valeur 1 dès qu'il y a coïncidence avec un mot de la phrase. \n",
    "\n",
    "= valeur numérique de 1 soit [1, 1, 1, 1]\n",
    "\n",
    "S'il faut convertir la phrase \"Python for machine learning\", 2 nouveaux mots sont à traiter.\n",
    "\n",
    "1- **Affectation des nouveaux codes :** machine=4 et learning=5, **ce qui correspond à l'opération de codage**.\n",
    "2- **Agrandissement du vecteur précédent pour accueillir les nouveaux mots :** [1, 1, 1, 1, 0, 0] \n",
    "3- **Calcul du vecteur pour la nouvelle chaîne :** [1, 1, 0, 0, 1, 1]\n",
    "\n",
    "Ce codage est assez efficace car il produit des vecteurs de caractéristiques bien ordonnés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8b6f258f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'python': 4, 'for': 1, 'data': 0, 'science': 5, 'machine': 3, 'learning': 2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "oh_encoder = CountVectorizer()\n",
    "oh_encoded = oh_encoder.fit_transform(['Python for data science', 'Python for machine learning'])\n",
    "\n",
    "print(oh_encoder.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5460bb",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "<br>\n",
    "\n",
    "## Matrices creuses et sélection déterministe\n",
    "\n",
    "Quand la plupart des valeurs de la matrices sont proches de 0, il est intéressant d'utiliser une matrice creuse. \n",
    "Une matrice creuse (sparse matrix) stocke pour toutes les cellules de la matrice les coordonnées des cellules et de leurs valeurs et non toute l'information. Lorsque l'application demande des données pour une cellule vide, la matrice renvoie la valeur zéro, après avoir cherché les coordonnées et n'avoir rien trouvé. \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## Calculer le temps d'exécution\n",
    "\n",
    "- `%timeit`: calcule le meilleur temps d'exécugtion d'une instruction (d'une ligne)\n",
    "- `%%timeit`: calcule le meilleur temps d'exécution de la séquence d'instructions d'une cellule.\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## Statistiques descriptives pour données numériques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c001bba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chargement du dataframe iris\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "iris = load_iris()\n",
    "\n",
    "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "iris_df['group'] = pd.Series([iris.target_names[k] for k in iris.target], dtype='category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba7b9a5",
   "metadata": {},
   "source": [
    "### Indicateurs de tendance centrale :\n",
    "- Moyenne \n",
    "- Médiane\n",
    "\n",
    "= permettent d'avoir une 1ère idée de la **centralisation des données et de leur symétrie**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "54c8f53a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sepal length (cm)    5.843333\n",
       "sepal width (cm)     3.057333\n",
       "petal length (cm)    3.758000\n",
       "petal width (cm)     1.199333\n",
       "dtype: float64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Moyenne\n",
    "iris_df.mean(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "51618366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sepal length (cm)    5.80\n",
       "sepal width (cm)     3.00\n",
       "petal length (cm)    4.35\n",
       "petal width (cm)     1.30\n",
       "dtype: float64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Médiane\n",
    "iris_df.median(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4177dbfb",
   "metadata": {},
   "source": [
    "Cette valeur médiane permet de localiser la position de la séparation entre les 2 moitiés de valeurs. La médiane est moins influencée par les cas anormaux ou par une distribution déséquilibrée des valeurs autour de la moyenne. \n",
    "\n",
    "## Variance, écart type et étendue\n",
    "\n",
    "Il s'agit ensuite de mesurer la **variance** ou plutôt sa racine carrée qui correspond à l'**écart type** (standard deviation). L'**écart type** donne autant d'informations que la variance, mais il est plus facile à comparer à la moyenne parce que l'unité de mesure est la même. La **variance** constitue un bon indicateur de l'intérêt d'une moyenne pour connaître la distribution des variables ; en effet, elle informe sur la façon dont les valeurs de la variable sont distribuées autour de la moyenne. Plus la **variance est élevée, plus loin peuvent se trouver certaines valeurs par rapport à la moyenne**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "72886e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sepal length (cm)    0.828066\n",
       "sepal width (cm)     0.435866\n",
       "petal length (cm)    1.765298\n",
       "petal width (cm)     0.762238\n",
       "dtype: float64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## écart type\n",
    "iris_df.std(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff3f527",
   "metadata": {},
   "source": [
    "On peut ensuite mesurer l'**étendue** (**range**) qui est la différence entre la valeur min/max pour chaque variable quantitative. Elle permet d'en apprendre plus au sujet des différences d'échelle entre les variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "daf7269a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm)    3.6\n",
      "sepal width (cm)     2.4\n",
      "petal length (cm)    5.9\n",
      "petal width (cm)     2.4\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(iris_df.max(numeric_only=True) - iris_df.min(numeric_only=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
